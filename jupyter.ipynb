{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None)\n"
     ]
    }
   ],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import sklearn \n",
    "from sklearn.datasets import load_files\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "cat = ['business','entertainment']\n",
    "#do not load filecontents\n",
    "datas =load_files('datasets/bbc/' ,categories=cat ,load_content=True ,shuffle=True, encoding='utf8', decode_error = 'strict' , random_state=0)                 \n",
    "\n",
    "tfidf = TfidfVectorizer(encoding='utf8', decode_error='strict', tokenizer=None, analyzer='word', stop_words=None ,ngram_range=(1, 1), max_df=1.0, min_df=1, max_features=None, vocabulary=None,binary=False, norm='l2', use_idf= True, smooth_idf=True,sublinear_tf=False )\n",
    "X = tfidf.fit_transform(datas.data)\n",
    "\n",
    "print(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def ensure_dir(file_path):\n",
    "    directory = os.path.dirname(file_path)\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "        \n",
    "if not os.path.exists(\"directory\"):\n",
    "    os.makedirs(\"directory\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 20 newsgroups dataset for categories:\n",
      "1251 documents\n",
      "5 categories\n",
      "*******************\n",
      "['/home/mabrin/project/cluster/datasets/bbcTest/politics/107.txt'\n",
      " '/home/mabrin/project/cluster/datasets/bbcTest/tech/055.txt'\n",
      " '/home/mabrin/project/cluster/datasets/bbcTest/business/154.txt' ...,\n",
      " '/home/mabrin/project/cluster/datasets/bbcTest/entertainment/225.txt'\n",
      " '/home/mabrin/project/cluster/datasets/bbcTest/entertainment/069.txt'\n",
      " '/home/mabrin/project/cluster/datasets/bbcTest/tech/156.txt']\n",
      "175\n",
      "*****************************************************\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'reshape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-e0eca003a251>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;31m#print(Y.shape , \">>>>                  >>>>>>>>>>>>>>>>\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcluster_centers_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcur\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;31m#left_doc , tmp = Y.shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'reshape'"
     ]
    }
   ],
   "source": [
    "\n",
    "from __future__ import print_function\n",
    "from collections import defaultdict\n",
    "import sklearn.datasets\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "import numpy as np\n",
    "from sklearn.cluster import Birch\n",
    "import sklearn\n",
    "\n",
    "categories = [\n",
    "   \"business\" ,\"sport\" , 'entertainment', 'tech','politics']\n",
    "print(\"Loading 20 newsgroups dataset for categories:\")\n",
    "#print(categories)\n",
    "\n",
    "dataset = sklearn.datasets.load_files('/home/mabrin/project/cluster/datasets/bbcTest/',\n",
    "                                      description=None, categories=categories ,\n",
    "                                      load_content=True, shuffle=True, encoding = 'utf-8',decode_error='ignore',\n",
    "                                      random_state=25)\n",
    "\n",
    "\n",
    "print(\"%d documents\" % len(dataset.data))\n",
    "print(\"%d categories\" % len(dataset.target_names))\n",
    "print(\"*******************\")\n",
    "#print(dataset.target)\n",
    "true_k = 5\n",
    "print(dataset.filenames)\n",
    "vectorizer = TfidfVectorizer(max_df=.3, max_features=175 ,\n",
    "                                 min_df=10\n",
    "                             , stop_words='english',\n",
    "                                 use_idf=True)\n",
    "#print(type(dataset.data))\n",
    "\n",
    "\n",
    "Y = vectorizer.fit_transform(dataset.data[125:])\n",
    "\n",
    "cur = Y.toarray()\n",
    "\n",
    "print( np.size(cur[1]))\n",
    "\n",
    "X = vectorizer.fit_transform(dataset.data[:125])\n",
    "\n",
    "km = MiniBatchKMeans(n_clusters=true_k, init='k-means++', n_init=1,init_size=1000, batch_size=1000)\n",
    "km.fit(X)\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "clusters = defaultdict(list)\n",
    "\n",
    "k = 0;\n",
    "for i in km.labels_ :\n",
    "  clusters[i].append(dataset.filenames[k])  \n",
    "  k += 1\n",
    "  \n",
    "'''for clust in clusters :\n",
    "  print (\"\\n************************\\n\")\n",
    "  for filename in clusters[clust] :\n",
    "    print (filename)'''\n",
    "print(\"*****************************************************\")\n",
    "\n",
    "#print(np.size(km.cluster_centers_[j]),\">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\")\n",
    "#print(Y.shape , \">>>>                  >>>>>>>>>>>>>>>>\")\n",
    "\n",
    "print(np.shape(km.cluster_centers_[1].reshape(1,-1)))\n",
    "print,np.shape(cur[1]).reshape(1,-1))\n",
    "\n",
    "#left_doc , tmp = Y.shape\n",
    "\n",
    "for i in range(0,left_doc):\n",
    "    clust = []\n",
    "    for j in range (5):\n",
    "        x = km.cluster_centers_[j]\n",
    "        y = cur[i]\n",
    "        clust.append(sklearn.metrics.pairwise.euclidean_distances(x.reshape(1,-1) , y.reshape(1,-1),\n",
    "                        Y_norm_squared=None, squared=False, X_norm_squared=None))\n",
    "    print(np.argmin(clust))\n",
    "    print(i)\n",
    "\n",
    "order_centroids = km.cluster_centers_.argsort()[:, ::-1]\n",
    "\n",
    "terms = vectorizer.get_feature_names()\n",
    "'''for i in range(true_k):\n",
    "        print(\"Cluster %d:\" % i, end='')\n",
    "        for ind in order_centroids[i, :200]:\n",
    "            print(' %s' % terms[ind], end='')\n",
    "        print()'''\n",
    "\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
